_target_: MLLM.src.models_clm.peft_models.peft_model_animegamer
model:
  _target_: MLLM.src.models.mistral.modeling_mistral.MistralForCausalLM.from_pretrained
  pretrained_model_name_or_path: ./checkpoints/Mistral-7B-Instruct-v0.1 # Path to Mistral-7B-Instruct-v0.1
  low_cpu_mem_usage: True
peft_config:
  _target_: peft.LoraConfig
  _convert_: object
  r: 32
  lora_alpha: 32
  modules_to_save:
    - input_layernorm
    - post_attention_layernorm
    - norm
  target_modules: 
    - q_proj 
    - v_proj 
    - k_proj 
    - o_proj 
    - gate_proj 
    - down_proj 
    - up_proj
  task_type: CAUSAL_LM
  lora_dropout: 0.05

vocab_size: 32284
